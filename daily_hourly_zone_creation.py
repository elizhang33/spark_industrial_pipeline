# -*- coding: utf-8 -*-
"""daily-hourly-zone-creation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/embedded/projects/nyc-taxi-analytics-479702/locations/us-west1/repositories/24d86120-44aa-412d-a552-a2edc4b0383b
"""

from google.cloud.dataproc_spark_connect import DataprocSparkSession
from google.cloud.dataproc_v1 import Session

# This will create a default Spark session
spark = DataprocSparkSession.builder.getOrCreate()
# If you would like to customize the Spark session, please refer to the
# documentation at https://cloud.google.com/bigquery/docs/use-spark
#
# For example, if you need to use a different subnetwork, use the code below

# from google.cloud.dataproc_v1 import Session

# session = Session()
# session.environment_config.execution_config.subnetwork_uri = "<subnetwork_name>"
# spark = DataprocSparkSession.builder.dataprocSessionConfig(session).getOrCreate()

# --- Configuration Check and Staging ---

# 1. VERIFY: The BQ connector is critical. Check if the template loaded it (often via 'spark.jars.packages').
# If you need to re-run the initialization, use the code below in a new cell:
from pyspark.sql import SparkSession
# Replace this with the actual version used in the template if different
BQ_CONNECTOR_PACKAGE = "com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.36.1"

# If Spark is already running, skip this and move to the 'spark.conf.set' line.
# If you needed to re-initialize:
# spark = (
#     SparkSession.builder
#     .appName("NYCTaxiSpark")
#     .config("spark.jars.packages", BQ_CONNECTOR_PACKAGE)
#     .getOrCreate()
# )

# 2. SET GCS STAGING BUCKET (Mandatory for BQ writes)
# Replace this with your actual GCS bucket name in the nyc-taxi-analytics project
GCS_BUCKET = "nyc-taxi-analytics-479702-temp-staging"
spark.conf.set("temporaryGcsBucket", GCS_BUCKET)

print("✅ Spark is ready to read/write to BigQuery.")

df = spark.read.format("bigquery") \
    .option("table", "nyc-taxi-analytics-479702.taxi_analytics.yellow_trips_covid") \
    .load()

df.printSchema()
df.show(5)

from pyspark.sql.functions import col, avg

df.select(
    avg("trip_distance"),
    avg("total_amount"),
    avg("trip_duration_min")
).show()

from pyspark.sql.functions import col, count, avg, sum as spark_sum

daily_stats = (
    df.groupBy("pickup_date", "covid_phase")
      .agg(
          count("*").alias("trips"),
          avg("trip_distance").alias("avg_distance"),
          avg("trip_duration_min").alias("avg_trip_duration_min"),
          avg("total_amount").alias("avg_total_amount"),
          spark_sum("total_amount").alias("total_revenue")
      )
      .orderBy("pickup_date")
)

daily_stats.show(20)

hourly_stats = (
    df.groupBy("pickup_hour", "covid_phase")
      .agg(
          count("*").alias("trips"),
          avg("trip_distance").alias("avg_distance"),
          avg("total_amount").alias("avg_fare")
      )
      .orderBy("pickup_hour", "covid_phase")
)

hourly_stats.show(30)

zones = (
    spark.read.format("bigquery")
        .option("table", "bigquery-public-data.new_york_taxi_trips.taxi_zone_geom")
        .load()
        .select("zone_id", "zone_name", "borough", "zone_geom")
)

joined = (
    df.join(
        zones,
        df.pickup_location_id == zones.zone_id,
        "left"
    )
    .select(
        "pickup_date",
        "covid_phase",
        "borough",
        "zone_id",
        "zone_name",
        "zone_geom",
        "trip_distance",
        "total_amount"
    )
)

# Aggregate by zone/date (no centroid confusion)

from pyspark.sql.functions import count, avg

zone_daily = (
    joined.groupBy(
        "pickup_date",
        "covid_phase",
        "borough",
        "zone_id",
        "zone_name",
        "zone_geom"
    )
    .agg(
        count("*").alias("rides"),
        avg("total_amount").alias("avg_fare"),
        avg("trip_distance").alias("avg_distance")
    )
)

# centroids for Power BI pin maps
zones_with_centroid = (
    spark.read.format("bigquery")
        .option("query", """
            SELECT
                zone_id,
                zone_name,
                borough,
                zone_geom,
                ST_X(ST_CENTROID(zone_geom)) AS longitude,
                ST_Y(ST_CENTROID(zone_geom)) AS latitude
            FROM `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom`
        """)
        .load()
)

# Convert to Pandas for plotting
daily_pd = daily_stats.toPandas()
daily_pd.head()

import matplotlib.pyplot as plt
import pandas as pd

daily_pd["pickup_date"] = pd.to_datetime(daily_pd["pickup_date"])

# Plot daily trips by COVID phase

plt.figure(figsize=(14, 6))

for phase in sorted(daily_pd["covid_phase"].unique()):
    subset = daily_pd[daily_pd["covid_phase"] == phase]
    plt.plot(subset["pickup_date"], subset["trips"], label=phase)

plt.title("NYC Yellow Taxi Daily Trips (2018–2022) by COVID Phase")
plt.xlabel("Date")
plt.ylabel("Trips per Day")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# Plot average total_amount over time

plt.figure(figsize=(14, 6))
plt.plot(daily_pd["pickup_date"], daily_pd["avg_total_amount"])
plt.title("Average Fare per Trip Over Time")
plt.xlabel("Date")
plt.ylabel("Avg Total Amount (USD)")
plt.grid(True)
plt.tight_layout()
plt.show()

daily_stats.write.format("bigquery") \
    .option("table", "nyc-taxi-analytics-479702.taxi_analytics.daily_stats") \
    .mode("overwrite") \
    .save()

hourly_stats.write.format("bigquery") \
    .option("table", "nyc-taxi-analytics-479702.taxi_analytics.hourly_stats") \
    .mode("overwrite") \
    .save()

zone_daily.write.format("bigquery") \
    .option("table", "nyc-taxi-analytics-479702.taxi_analytics.zone_daily_stats") \
    .mode("overwrite") \
    .save()

"""ML Model: Demand Forecasting + Anomaly Detection"""

# Make sure it's sorted
daily_pd = daily_pd.sort_values("pickup_date")

# Focus on a simpler subset: total trips per day (aggregate over covid_phase)
daily_total = (
    daily_pd.groupby("pickup_date", as_index=False)["trips"].sum()
            .rename(columns={"trips": "total_trips"})
)

daily_total.head()

train = daily_total[daily_total["pickup_date"] < "2022-01-01"].copy()
test  = daily_total[daily_total["pickup_date"] >= "2022-01-01"].copy()

# Baseline forecasting model - scikit-learn (RandomForestRegressor) with calendar features

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error
import numpy as np

def add_time_features(df):
    df = df.copy()
    df["dayofweek"] = df["pickup_date"].dt.dayofweek
    df["month"] = df["pickup_date"].dt.month
    df["year"] = df["pickup_date"].dt.year
    df["dayofyear"] = df["pickup_date"].dt.dayofyear
    return df

train_feat = add_time_features(train)
test_feat  = add_time_features(test)

features = ["dayofweek", "month", "year", "dayofyear"]
X_train = train_feat[features]
y_train = train_feat["total_trips"]
X_test  = test_feat[features]
y_test  = test_feat["total_trips"]

model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
print("Test MAE:", mae)

plt.figure(figsize=(14,6))
plt.plot(test_feat["pickup_date"], y_test.values, label="Actual")
plt.plot(test_feat["pickup_date"], y_pred, label="Predicted")
plt.title("Taxi Demand Forecasting: Actual vs Predicted (2022)")
plt.xlabel("Date")
plt.ylabel("Total Trips")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Detect days where the number of trips is “weird” compared to others

from sklearn.ensemble import IsolationForest

# Use multiple features for anomaly detection
feat_df = add_time_features(daily_total)
X = feat_df[["total_trips", "dayofweek", "month", "dayofyear"]]

iso = IsolationForest(contamination=0.01, random_state=42)
feat_df["anomaly_score"] = iso.fit_predict(X)  # -1 = anomaly, 1 = normal

anomalies = feat_df[feat_df["anomaly_score"] == -1]
anomalies.head()

plt.figure(figsize=(14,6))
plt.plot(feat_df["pickup_date"], feat_df["total_trips"], label="Total trips", alpha=0.7)
plt.scatter(
    anomalies["pickup_date"],
    anomalies["total_trips"],
    color="red",
    label="Anomaly",
    zorder=5
)
plt.title("Anomalous Days in NYC Taxi Demand")
plt.xlabel("Date")
plt.ylabel("Total Trips")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()